{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37088,"sourceType":"datasetVersion","datasetId":29101},{"sourceId":6732149,"sourceType":"datasetVersion","datasetId":3877552},{"sourceId":6959220,"sourceType":"datasetVersion","datasetId":3997576},{"sourceId":6972250,"sourceType":"datasetVersion","datasetId":4006034},{"sourceId":7076028,"sourceType":"datasetVersion","datasetId":4044689},{"sourceId":7750134,"sourceType":"datasetVersion","datasetId":3880077}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SVD-Grad-CAM\n","metadata":{"id":"VPz1RlDzUQL0"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport keras\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv2D, Conv3D, DepthwiseConv2D, SeparableConv2D, Conv3DTranspose\nfrom keras.layers import Flatten, MaxPool2D, AvgPool2D, GlobalAvgPool2D, UpSampling2D, BatchNormalization\nfrom keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation, LeakyReLU, PReLU\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\nfrom sklearn.decomposition import PCA\nfrom keras import regularizers\nfrom IPython.display import SVG\n#from keras.utils.vis_utils import model_to_dot\nfrom keras.applications.densenet import DenseNet169, DenseNet121, preprocess_input\nfrom time import time\n\nimport datetime","metadata":{"id":"mkjRfywGQWRn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def listdir_nohidden(path):\n    '''\n    Utility function to find the list of files in a directory excluding the hidden files.\n    Args:\n        path: contains the path of the directory containing the images\n\n    '''\n    for f in os.listdir(path):\n        if not f.startswith('.'):\n            yield f","metadata":{"id":"-sa-ji_TQmmV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_image_df = pd.read_csv('/kaggle/input/testing/valid_image_data.csv', names=['Path','Count', 'Label'])\npositives =0\ni=0\nfor i, data in tqdm(valid_image_df.iterrows()):\n    positives += data['Label']\n    i+=1\n    if i == 250:\n        break\npositives\ndfs=valid_image_df['Path']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DenseNet Model","metadata":{"id":"Rc2WUD9ATuot"}},{"cell_type":"code","source":"from tensorflow.keras.applications import DenseNet169,InceptionV3,ResNet50\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\n\ndef DenseNetModel(n_classes):\n    base_model = DenseNet169(input_shape=(224, 224, 3),\n                          weights='imagenet',\n                          include_top=False)\n\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)  # Global Average Pooling\n\n    predictions = Dense(n_classes, activation='sigmoid')(x)  # Softmax activation for classification\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model\n\nn_classes = 1  # Number of classes for ImageNet dataset\nmodel = DenseNetModel(1)\n#model.summary()\nlayerName = 'relu'","metadata":{"id":"-5eDwQR4E8i9","outputId":"f0acdcc3-2e13-4917-a851-b2ca22d44c88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/my-model/MURA_MY_DenseNet-ELBOW.h5\")","metadata":{"id":"6Mf2ajBiPu0N","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VISUAL EXPLANATIONS\n**Step 1: Image Preprocessing**","metadata":{"id":"5t1WEOfJTqiE"}},{"cell_type":"code","source":"def getImage2(path, size):\n    '''\n    Function to process the images\n    Args:\n        dataframe: contains the path to the images in the directory\n        size: contains the value to which the shape of the image will resized\n    '''\n    Images = []\n    try:\n        image = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n         # clipLimit -> Threshold for contrast limiting\n        clahe = cv2.createCLAHE(clipLimit = 3,tileGridSize=(3,3))\n        image = clahe.apply(image)\n       # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        print(image.shape)\n        # threshold\n        thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n        hh, ww = thresh.shape\n        print(thresh.shape)\n        # make bottom 2 rows black where they are white the full width of the image\n        thresh[hh-3:hh, 0:ww] = 0\n\n        # get bounds of white pixels\n        white = np.where(thresh==255)\n        xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n        #print(xmin,xmax,ymin,ymax)\n\n        # crop the image at the bounds adding back the two blackened rows at the bottom\n        crop = image[ymin:ymax+3, xmin:xmax]\n        crop = cv2.resize(crop,(size,size))\n        # Normalixation\n\n        #crop=crop/255\n\n        #image = randome_rotation_flip(image,size)\n        print(crop.shape)\n        rgb_array = np.stack((crop,) * 3, axis=-1)\n        Images.append(rgb_array)\n\n\n    except Exception as e:\n        print(str(e))\n\n    Images = np.asarray(Images).astype('float32')\n\n   # mean = np.mean(Images)\n   # std = np.std(Images)\n    Images = Images/255\n\n    #if K.image_data_format() == \"channels_first\":\n    #plt.imshow(Images[0]) #Extended dimension 3\n    #if K.image_data_format() == \"channels_last\":\n    #  Images = np.expand_dims(Images,axis=3)             #Extended dimension 3(usebackend tensorflow:aixs=3; theano:axixs=1)\n    return Images\n","metadata":{"id":"Qng0HTgVjBB2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choose an Image\n","metadata":{"id":"gaoODaqiw8mw"}},{"cell_type":"code","source":"import pandas as pd\nhelps= {'Path': ['/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11815/study1_positive/image2.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11756/study1_positive/image3.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11389/study1_positive/image4.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11823/study1_positive/image2.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11832/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11839/study1_positive/image3.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11842/study1_positive/image5.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11823/study1_positive/image2.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11832/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11472/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11603/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11780/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11326/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11326/study1_positive/image3.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11815/study1_positive/image2.png','/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11186/study1_positive/image2.png']}\ndfs=pd.DataFrame(helps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path='/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11372/study1_negative/image1.png'\nimage = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\nclahe = cv2.createCLAHE(clipLimit = 3,tileGridSize=(3,3))\nimage = clahe.apply(image)\n\nfinal_img = getImage2(img_path,224)\n#x=final_img[0]\nplt.imshow(final_img[0])\nprint(final_img.shape)","metadata":{"id":"MrUAe9t8xABM","outputId":"70d5c948-53f6-45ca-f32c-960f33c38b34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 3 : to get SuperImposition of Class Activation Mappings on Input Image**","metadata":{"id":"yC71Lv6RtnQl"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import Model\nimport cv2\nfrom tensorflow.keras.preprocessing import image\nfrom PIL import Image\n\ndef show_imgwithheat(img_path, heatmap, alpha=0.4, return_array=False):\n    \"\"\"Show the image with heatmap.\n\n    Args:\n        img_path: string.\n        heatmap: image array, get it by calling grad_cam().\n        alpha: float, transparency of heatmap.\n        return_array: bool, return a superimposed image array or not.\n    Return:\n        None or image array.\n    \"\"\"\n    image = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n    # clipLimit -> Threshold for contrast limiting\n    clahe = cv2.createCLAHE(clipLimit = 3,tileGridSize=(3,3))\n    image = clahe.apply(image)\n    # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    #print(heatmap.shape)\n    #plt.imshow(heatmap)\n    # threshold\n    thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n    hh, ww = thresh.shape\n    # make bottom 2 rows black where they are white the full width of the image\n    thresh[hh-3:hh, 0:ww] = 0\n\n    # get bounds of white pixels\n    white = np.where(thresh==255)\n    xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n        #print(xmin,xmax,ymin,ymax)\n\n        # crop the image at the bounds adding back the two blackened rows at the bottom\n    crop = image[ymin:ymax+3, xmin:xmax]\n    img = cv2.resize(crop,(224,224))\n\n #Images = np.expand_dims(Images,axis=3) #Extended dimension 3\n   # display(np.expand_dims(img,axis=2))\n  #  img = cv2.imread(img_path)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = (heatmap*255).astype(\"uint8\")\n   # cv2.apply\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = heatmap * alpha + cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n\n    imgwithheat = Image.fromarray(superimposed_img)\n    try:\n        display(imgwithheat)\n    except NameError:\n        imgwithheat.show()\n\n    if return_array:\n        return superimposed_img\n\ndef prediction_label(p):\n    if(p > 0.5):\n        return 1\n    else:\n        return 0\n","metadata":{"id":"s1Sm8AjIip7G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Last Conv Layer\n\nResnet50 'conv5_block3_out'\n\nInceptionV3 'mixed10'\n\nDensenet169 'relu'","metadata":{}},{"cell_type":"code","source":"layerName='relu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GRAD-CAM**","metadata":{"id":"JZNcLwrAuBHq"}},{"cell_type":"code","source":" import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom sklearn.decomposition import TruncatedSVD\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tensorflow.keras.applications.imagenet_utils import decode_predictions\n\ndef svd_grad_cam(model, img, layer_name=\"relu\", top_k=5, n_components=1):\n    \"\"\"\n    Get heatmaps by Grad-CAM with SVD for the top K predicted classes.\n\n    Args:\n        model: A model object, built from tf.keras 2.X.\n        img: An image ndarray.\n        layer_name: A string, layer name in the model.\n        top_k: An integer, number of top classes to generate heatmaps for.\n        n_components: Number of components to keep in the SVD.\n\n    Returns:\n        A tuple of heatmaps and decoded predictions.\n    \"\"\"\n    img_tensor = np.expand_dims(img, axis=0)\n\n    conv_layer = model.get_layer(layer_name)\n    heatmap_model = Model([model.inputs], [conv_layer.output, model.output])\n\n    # Making the GradientTape persistent\n    with tf.GradientTape(persistent=True) as gtape:\n        conv_output, predictions = heatmap_model(img_tensor[0])\n        sorted_indices = np.argsort(np.squeeze(predictions))[::-1]\n        top_k_indices = sorted_indices[:top_k]\n\n        heatmaps = np.zeros((top_k, conv_output.shape[1], conv_output.shape[2]))\n\n        for i, category_id in enumerate(top_k_indices):\n            output = predictions[:, category_id]\n            grads = gtape.gradient(output, conv_output)\n            grads_flat = np.array(grads).flatten().reshape(1664, 7, 7)\n            grads_flat[np.isnan(grads_flat)] = 0\n\n            X_svd = []\n            \n\n            for j in range(1664):\n                s, u, vt = tf.linalg.svd(grads_flat[j], full_matrices=True)\n                u = u[:, :2]\n                s_top1 = s[:2]\n                vt = vt[:2, :]\n                s_top1_diag = tf.linalg.diag(s_top1)\n                reconstructed_matrix = tf.matmul(u, tf.matmul(s_top1_diag, vt))\n                X_svd.append(tf.reduce_mean(reconstructed_matrix))\n\n            heatmap = tf.reduce_mean(tf.multiply(np.array(X_svd), conv_output), axis=-1)\n            heatmap = np.maximum(heatmap, 0)\n            max_heat = np.max(heatmap)\n            if max_heat == 0:\n                max_heat = 1e-10\n            heatmap /= max_heat\n            heatmaps[i, :, :] = np.squeeze(heatmap)\n\n    # Deleting the tape after use since it's persistent\n    del gtape\n\n    decoded_predictions = decode_predictions(np.array(predictions), top=top_k)[0]\n    return heatmaps, decoded_predictions\n\n\n# Example usage:\nheatmaps, decoded_predictions = svd_grad_cam(model, img_array, layer_name='relu', top_k=5)\n","metadata":{"id":"5HBB8cEoih7q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path='/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11375/study1_negative/image2.png'\nimge = final_img = getImage3(img_path,224)\n#x=final_img[0]\nprint(final_img.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad_cams=grad_cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\ngrad_cam_superimposed = imgwithheat(img_path, grad_cams[0])\ngcc =coherency(grad_cams[0],getImage3(imge,224,1,grad_cam_superimposed),model,grad_cam,grad_cams[1])\n\nguass_cam = svd__cam(model,final_img,label_name=['negative','positive'],layer_name=layerName)\nguass_cam_superimposed = imgwithheat(img_path,guass_cam[0])\nggc=coherency(guass_cam[0],getImage3(imge,224,1,guass_cam_superimposed),model,svd__cam,guass_cam[1])\n\nsmooth_cam=eigen_cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\nsmooth_superimposed = imgwithheat(img_path, smooth_cam[0])\nsgc=coherency(smooth_cam[0],getImage3(imge,224,1,smooth_superimposed),model,eigen_cam,smooth_cam[1])\n\nscore_cams=score_camp(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\nprint(\"00\",score_cams[0].shape)\nscore_cam_superimposed = imgwithheat(img_path,score_cams[0])\nsc=coherency(score_cams[0],getImage3(imge,224,1,score_cam_superimposed),model,score_camp,score_cams[1])\n\nfig, ax = plt.subplots(nrows=1,ncols=5, figsize=(18, 11))\nax[0].imshow(final_img[0])\nax[0].set_title(\"input image\")\nax[0].annotate(\"Coherency\", xy=(0.5, -0.1), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=10)\nax[1].imshow(grad_cam_superimposed)\nax[1].annotate(\"Coherency: \"+tf.strings.as_string(gcc[2]).numpy().decode('utf-8'), xy=(0.5, -0.1), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[1].set_title(\"Grad-CAM\")\nax[2].imshow(smooth_superimposed)\nax[2].annotate(\"Coherency: \"+tf.strings.as_string(sgc[2]).numpy().decode('utf-8'), xy=(0.5, -0.1), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[2].set_title(\"Eigen_cam\")\nax[3].imshow(guass_cam_superimposed)\nax[3].annotate(\"Coherency: \"+tf.strings.as_string(ggc[2]).numpy().decode('utf-8'), xy=(0.5, -0.1), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[3].set_title(\"Grad_new-CAM\")\nax[4].imshow(score_cam_superimposed)\nax[4].annotate(\"Coherency: \"+tf.strings.as_string(sc[2]).numpy().decode('utf-8'), xy=(0.5, -0.1), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[4].set_title(\"Score-Cam\")\nfor j in range(5):\n    ax[j].axis('off')\nplt.show()\nfig.savefig('Coherency_DenseNet169')","metadata":{"id":"jm_tYVRfG6Fy","outputId":"e1eed123-424c-4ce0-ed95-3aa15fd5311c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# QUALITATIVE EVALUATION OF CAM TECHNIQUES","metadata":{}},{"cell_type":"markdown","source":"# **Average Drop.**\nIt measures the average percentage drop in\nconfidence for the target class c when the model sees only\nthe explanation map, instead of the full image. For one im-\nage, the metric is defined as \n> ***(max(0, yc − oc)/yc) · 100***\n\nwhere yc is the output score for class c when using the full\nimage, and oc the output score when using the explanation\nmap. The value is then averaged over a set of images.\n# **Average Increase.** \nIt computes, instead, the number of\ntimes the confidence of the model is higher when using the\nexplanation map compared to when using the entire image.\nFormally, for a single image it is defined as \n> ***1✶yc<oc · 100***\n\nwhere ✶ is the indicator function. The value is then again\naveraged over different images.\nInsertion and Deletion. Deletion ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef average_drop(final_img, explanation_map, model, out, class_idx=None):\n    # print(explanation_map.shape)\n    # Get the output of the neural network on the explanation map\n    out_on_exp = model.predict(explanation_map)[0]\n    \n    # Get the confidence of the correct class in the original input image\n    confidence_on_inp = out[0]\n\n    # Get the confidence of the correct class in the image after applying the explanation map\n    confidence_on_exp = out_on_exp[0]\n\n    # Compute the average drop in confidence\n    average_drop = tf.math.maximum(0., confidence_on_inp - confidence_on_exp) / confidence_on_inp\n    average_inc = 1 if ((confidence_on_inp - confidence_on_exp) < 0) else 0\n    #print(average_drop,average_inc)\n    return [average_drop, average_inc]","metadata":{"id":"N8CbPAnBzdZz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pearson_corr(saliency_map_A, saliency_map_B):\n    # Compute the mean of each saliency map\n    mean_A = tf.math.reduce_mean(saliency_map_A)\n    mean_B = tf.math.reduce_mean(saliency_map_B)\n\n    # Center the saliency maps\n    saliency_map_A_centered = saliency_map_A - mean_A\n    saliency_map_B_centered = saliency_map_B - mean_B\n    \n    # Compute the standard deviation of each saliency map\n    std_A = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(saliency_map_A_centered)))\n    std_B = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(saliency_map_B_centered)))\n\n    # Compute the numerator of the Pearson correlation coefficient\n    numerator = tf.reduce_mean(saliency_map_A_centered * saliency_map_B_centered - std_A*std_A*std_B*std_B)\n\n    # Compute the denominator of the Pearson correlation coefficient\n    denominator = std_A * std_B\n\n    # Compute the Pearson correlation coefficient\n    if std_A ==0 or std_B ==0:\n        pearson_corr=numerator\n    else:\n        pearson_corr = numerator / denominator\n\n    return pearson_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Maximum Coherency**\nThe CAM should contain all the\nrelevant features that explain a prediction and should re-\nmove useless features in a coherent way. As a consequence,\ngiven an input image x and a class of interest c, the CAM\nof x should not change when conditioning x on the CAM\nitself. Formally,\n> ***CAMc(x ⊙ CAMc(x)) = CAMc(x)***\n\nNotice that this is equivalent to requiring that the CAM of\none image should be equal to that of the explanation map\nobtained with the same CAM approach. To measure the\nextent to which an approach satisfies the coherency prop-\nerty, we define a metric that measures how much the CAM\nchanges when smoothing pixels with a low attribution score.\nFollowing previous works in the comparison of saliency\nmaps [24, 5, 6, 2, 7, 8], we use the Pearson Correlation Co-\nefficient between the two CAMs considered in Eq. 2:\n> ***Coherency(x) = Cov(CAMc(x ⊙ CAMc(x)), CAMc(x)) / σCAMc(x⊙CAMc(x))σCAMc(x)***","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras\nfrom tensorflow.keras import layers,backend\ndef coherency(saliency_map, explanation_map, model, attr_method,inp_score=[1]):\n    # Convert explanation map to a tensor suitable for Keras\n    # Compute the saliency map for the explanation map\n    saliency_map_B = attr_method(model,explanation_map,label_name = ['negative','positive'],layer_name=layerName)\n    # Flatten the saliency maps\n    drop = max(0,inp_score[0]-saliency_map_B[1][0])\n    inc=1\n    if drop > 0 :\n        inc=0\n    #print(saliency_map.shape,saliency_map_B[0].shape)\n    saliency_map_A2 = tf.reshape(saliency_map, [-1])\n    saliency_map_B2 = tf.reshape(saliency_map_B[0], [-1])\n    #print(saliency_map_B[0].shape,saliency_map_B2.shape)\n    # Compute the Pearson correlation coefficient\n    pearso_corr = pearson_corr(saliency_map_A2, saliency_map_B2)\n    #print(pearso_corr)\n    # Normalize the correlation coefficient to be between 0 and 1\n    normalized_corr = (pearso_corr + 1) / 2\n\n    return [drop,inc,normalized_corr]","metadata":{"id":"JNq9Zj4LAGDM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Minimum Complexity**\n Beyond requiring that the CAM\nshould be coherent in removing features from the input im-\nage, we must also require it to be as less complex as possi-\nble, i.e., it must contain the minimum set of pixels that ex-\nplains the prediction. Employing the L1 norm as a proxy of\nthe complexity of a CAM, we define the Complexity mea-\nsure as:\n> ***Complexity(x) = ‖CAMc(x)‖1***\n\nComplexity is minimized when the number of pixels high-\nlighted by the attribution method is low.","metadata":{}},{"cell_type":"code","source":"def complexity(saliency_map):\n    return abs(saliency_map).sum()/(saliency_map.shape[-1]*saliency_map.shape[-2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results=outs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results={'cam': {\"Avg Drop\": 0,\"Avg Increase\":0,\"Coherency\":0,\"Complexicity\" : 0},\n             'grad-cam': {\"Avg Drop\": 0,\"Avg Increase\":0,\"Coherency\":0,\"Complexicity\" : 0},\n             'scorecam': {\"Avg Drop\": 0,\"Avg Increase\":0,\"Coherency\":0,\"Complexicity\" : 0},\n             'svd_grad_cam':{\"Avg Drop\": 0,\"Avg Increase\":0,\"Coherency\":0,\"Complexicity\" : 0},\n             'eigen_cam':{\"Avg Drop\": 0,\"Avg Increase\":0,\"Coherency\":0,\"Complexicity\" : 0}\n            }\ndef Meterics(dataframe, size):\n    '''\n    Function to process the images\n    Args:\n        dataframe: contains the path to the images in the directory\n        size: contains the value to which the shape of the image will resized\n    '''\n    ko=0\n    for i, data in tqdm(dataframe.iterrows()):\n        imge=data['Path']\n        print(imge)\n        final_image = getImage3(imge,224)\n        #heatmaps\n        heatmap1=cam(model, final_image,\n                   label_name = ['negative','positive'],layer_name=layerName)\n        heatmap2=grad_cam(model, final_image,\n                   label_name = ['negative','positive'],layer_name=layerName)\n        heatmap3=score_camp(model, final_image,\n                   label_name = ['negative','positive'],layer_name=layerName)\n        heatmap4 = svd_grad_cam(model,final_image,\n                           label_name=['negative','positive'],layer_name=layerName)\n        heatmap5=eigen_cam(model,final_image,\n                          label_name=['negative','positive'],layer_name=layerName)\n        #Explnation_maps\n        explnationmap1=imgwithheat(imge, heatmap1[0])\n        explnationmap2=imgwithheat(imge, heatmap2[0])\n        explnationmap3=imgwithheat(imge, heatmap3[0])\n        explnationmap4=imgwithheat(imge, heatmap4[0])\n        explnationmap5=imgwithheat(imge, heatmap5[0])\n\n        preproccessed1=getImage3(imge,224,1,explnationmap1)\n        preproccessed2=getImage3(imge,224,1,explnationmap2)\n        preproccessed3=getImage3(imge,224,1,explnationmap3)\n        preproccessed4=getImage3(imge,224,1,explnationmap4)\n        preproccessed5=getImage3(imge,224,1,explnationmap5)\n        #show_imgwithheat(imge,heatmap[0])\n\n        avg=coherency(heatmap1[0],preproccessed1,model,cam,heatmap1[1])\n        results['cam'][\"Avg Drop\"]+=np.array(avg[0])\n        results['cam'][\"Avg Increase\"]+=avg[1]*100/size\n        results['cam'][\"Coherency\"]+=np.array(avg[2])*100/size\n        results['cam'][\"Complexicity\"]+=np.array(complexity(heatmap1[0]))*100/size\n\n        avg=coherency(heatmap2[0],preproccessed2,model,grad_cam,heatmap2[1])\n        results['grad-cam'][\"Avg Drop\"]+=np.array(avg[0])\n        results['grad-cam'][\"Avg Increase\"]+=avg[1]*100/size\n        results['grad-cam'][\"Coherency\"]+=np.array(avg[2])*100/size\n        results['grad-cam'][\"Complexicity\"]+=np.array(complexity(heatmap2[0]))*100/size\n\n        avg=coherency(heatmap3[0],preproccessed3,model,score_camp,heatmap3[1])\n        results['scorecam'][\"Avg Drop\"]+=np.array(avg[0])\n        results['scorecam'][\"Avg Increase\"]+=avg[1]*100/size\n        results['scorecam'][\"Coherency\"]+=np.array(avg[2])*100/size\n        results['scorecam'][\"Complexicity\"]+=np.array(complexity(heatmap3[0]))*100/size\n\n        avg=coherency(heatmap4[0],preproccessed4,model,svd_grad_cam,heatmap4[1])\n        results['svd_grad_cam'][\"Avg Drop\"]+=np.array(avg[0])\n        results['svd_grad_cam'][\"Avg Increase\"]+=avg[1]*100/size\n        results['svd_grad_cam'][\"Coherency\"]+=np.array(avg[2])*100/size\n        results['svd_grad_cam'][\"Complexicity\"]+=np.array(complexity(heatmap4[0]))*100/size\n\n        avg=coherency(heatmap5[0],preproccessed5,model,eigen_cam,heatmap5[1])\n        results['eigen_cam'][\"Avg Drop\"]+=np.array(avg[0])\n        results['eigen_cam'][\"Avg Increase\"]+=avg[1]*100/size\n        results['eigen_cam'][\"Coherency\"]+=np.array(avg[2])*100/size\n        results['eigen_cam'][\"Complexicity\"]+=np.array(complexity(heatmap5[0]))*10/size\n\n        ko+=1\n        if ko==size:\n            break\n    results['cam'][\"Avg Drop\"]=results['cam'][\"Avg Drop\"]*100/(size-results['cam'][\"Avg Increase\"]*size/100)\n    results['grad-cam'][\"Avg Drop\"]/=(size-results['grad-cam'][\"Avg Increase\"]*size/100)/100\n    results['scorecam'][\"Avg Drop\"]/=(size-results['scorecam'][\"Avg Increase\"]*size/100)/100\n    results['svd_grad_cam'][\"Avg Drop\"]/=(size-results['svd_grad_cam'][\"Avg Increase\"]*size/100)/100\n    results['eigen_cam'][\"Avg Drop\"]/=(size-results['eigen_cam'][\"Avg Increase\"]*size/100)/100\n    return results\nouts=Meterics(dfs,15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(outs)\n\n# Save the DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nhelps= {'Path': ['/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11815/study1_positive/image2.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11756/study1_positive/image3.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11389/study1_positive/image4.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11823/study1_positive/image2.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11832/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11839/study1_positive/image3.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11842/study1_positive/image5.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11823/study1_positive/image2.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11832/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11472/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11603/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11780/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11326/study1_positive/image1.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11326/study1_positive/image3.png',\n '/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11815/study1_positive/image2.png','/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11603/study1_positive/image1.png']}\ndfs=pd.DataFrame(helps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef insertion(y_true, y_pred, heatmap):\n    \"\"\"\n    Calculates the rise in the target class probability as pixels are added\n    according to the CAM.\n\n    Args:\n        y_true (np.ndarray): The ground truth class labels.\n        y_pred (np.ndarray): The predicted class labels.\n        heatmap (np.ndarray): The CAM heatmap.\n\n    Returns:\n        float: The rise in the target class probability.\n    \"\"\"\n    # Sort the heatmap in descending order\n    heatmap_sorted = np.sort(heatmap)\n\n    # Calculate the AUC for the full image\n    auc_full = roc_auc_score(y_true, y_pred)\n\n    # Calculate the AUC for each level of the heatmap\n    aucs = []\n    for threshold in heatmap_sorted:\n        y_pred_threshold = y_pred[heatmap >= threshold]\n        auc = roc_auc_score(y_true, y_pred_threshold)\n        aucs.append(auc)\n\n    # Calculate the insertion metric\n    insertion = auc_full - np.mean(aucs)\n    return insertion\n\ndef deletion(y_true, y_pred, heatmap):\n    \"\"\"\n    Calculates the drop in the probability of the target class as\n    important pixels (given by the CAM) are gradually removed from the image.\n\n    Args:\n        y_true (np.ndarray): The ground truth class labels.\n        y_pred (np.ndarray): The predicted class labels.\n        heatmap (np.ndarray): The CAM heatmap.\n\n    Returns:\n        float: The drop in the probability of the target class.\n    \"\"\"\n    # Sort the heatmap in ascending order\n    heatmap_sorted = np.sort(heatmap)[::-1]\n\n    # Calculate the AUC for the full image\n    auc_full = roc_auc_score(y_true, y_pred)\n\n    # Calculate the AUC for each level of the heatmap\n    aucs = []\n    for threshold in heatmap_sorted:\n        y_pred_threshold = y_pred[heatmap <= threshold]\n        auc = roc_auc_score(y_true, y_pred_threshold)\n        aucs.append(auc)\n\n    # Calculate the deletion metric\n    deletion = auc_full - np.mean(aucs)\n    return deletion\n","metadata":{"id":"jeAwYsXqr618","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insertion(1,smooth_cam[1],smooth_cam[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coh1 = [0,0,0,0]\ncoh2=[0,0,0,0]\ndef helper(final_img,img_path,ko):\n    \n    cams=cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n    cam_superimposed = imgwithheat(img_path, cams[0])\n    \n    grad_cams=grad_cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n    grad_cam_superimposed = imgwithheat(img_path, grad_cams[0])\n    \n    smooth_grad_cams=smoothgradNow(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n    smooth_grad_cam_superimposed = imgwithheat(img_path, smooth_grad_cams[0])\n    \n    score_cams=score_camp(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n    score_cam_superimposed = imgwithheat(img_path, score_cams[0])\n    \n    eigen_cams=eigen_cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n    eigen_cam_superimposed = imgwithheat(img_path,eigen_cams[0])\n\n    svd_grad_cams=svd_grad_cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n    svd_grad_superimposed = imgwithheat(img_path, svd_grad_cams[0])\n\n    fig, ax = plt.subplots(nrows=1,ncols=7, figsize=(20, 6))\n    ax[0].imshow(final_img[0])\n    ax[0].set_title(\"input image\")\n    ax[1].imshow(cam_superimposed)\n    ax[1].set_title(\"CAM\")\n    ax[2].imshow(grad_cam_superimposed)\n    ax[2].set_title(\"Grad-CAM\")\n    ax[3].imshow(smooth_grad_cam_superimposed)\n    ax[3].set_title(\"Smooth-Grad-CAM\")\n    ax[4].imshow(score_cam_superimposed)\n    ax[4].set_title(\"Score-CAM\")\n    ax[5].imshow(eigen_cam_superimposed)\n    ax[5].set_title(\"Eigen-CAM\")\n    ax[6].imshow(svd_grad_superimposed)\n    ax[6].set_title(\"SVD_Grad_CAM\")\n    for j in range(7):\n        ax[j].axis('off')\n    plt.show()\n    rc='SVD Evalation'+str(ko)+'.png'\n    fig.savefig(rc, bbox_inches='tight')","metadata":{"id":"G_ctf2GVq4B8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"helper(final_img,img_path,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getImagerp(dataframe, size):\n    '''\n    Function to process the images\n    Args:\n        dataframe: contains the path to the images in the directory\n        size: contains the value to which the shape of the image will resized\n    '''\n    ko=1\n    for i, data in tqdm(dataframe.iterrows()):\n        img=data['Path']\n        print(img)\n        final_ = getImage3(img,224)\n        helper(final_,img,ko)\n        ko+=1\ngetImagerp(dfs,15)","metadata":{"id":"LIRNm7nTrDHx","outputId":"93ae89dd-2211-42af-aca6-3003da8a6410","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coh1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coh2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Selected Examples\n\n/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11815/study1_positive/image2.png\n\n\n/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11756/study1_positive/image3.png\n\n/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11389/study1_positive/image4.png\n\n/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11823/study1_positive/image2.png\n\n/kaggle/input/my-mura/MURA-v1.1/valid/XR_ELBOW/patient11832/study1_positive/image1.png","metadata":{}},{"cell_type":"code","source":"cams = cam(model,final_img,label_name=['negative','positive'],layer_name=layerName)\ncam_superimposed = imgwithheat(img_path,cams[0])\nc=coherency(cams[0],getImage3(imge,224,1,cam_superimposed),model,cam,cams[1])\n\ngrad_cams = grad_cam(model,final_img,label_name=['negative','positive'],layer_name=layerName)\ngrad_cam_superimposed = imgwithheat(img_path,grad_cams[0])\ngc=coherency(grad_cams[0],getImage3(imge,224,1,grad_cam_superimposed),model,grad_cam,grad_cams[1])\n\nsmooth_cam=smoothgradNow(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\nsmooth_superimposed = imgwithheat(img_path, smooth_cam[0])\nsgc=coherency(smooth_cam[0],getImage3(imge,224,1,smooth_superimposed),model,smoothgradNow,smooth_cam[1])\n\nscore_cams=score_camp(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\n#print(\"00\",score_cams[0].shape)\nscore_cam_superimposed = imgwithheat(img_path,score_cams[0])\nsc=coherency(score_cams[0],getImage3(imge,224,1,score_cam_superimposed),model,score_camp,score_cams[1])\n\neigen_cams=eigen_cam(model,final_img,label_name = ['negative','positive'],layer_name=layerName)\neigen_cam_superimposed = imgwithheat(img_path,eigen_cams[0])\nec=coherency(eigen_cams[0],getImage3(imge,224,1,eigen_cam_superimposed),model,eigen_cam,score_cams[1])\n\nsvd_grad_cams = svd_grad_cam(model,final_img,label_name=['negative','positive'],layer_name=layerName)\nsvd_grad_cam_superimposed = imgwithheat(img_path,svd_grad_cams[0])\nsvgc=coherency(svd_grad_cams[0],getImage3(imge,224,1,svd_grad_cam_superimposed),model,svd_grad_cam,svd_grad_cams[1])\n\nfig, ax = plt.subplots(nrows=1,ncols=7, figsize=(20, 13))\nax[0].imshow(final_img[0])\nax[0].annotate(\"Input Image\", xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[1].imshow(cam_superimposed)\nax[1].annotate(\"Max Coherency: \"+tf.strings.as_string(c[2]).numpy().decode('utf-8')+\"\\nAvg Drop: \" +tf.strings.as_string(c[0]).numpy().decode('utf-8') + \"\\nAvg Increase: \" + tf.strings.as_string(c[1]).numpy().decode('utf-8'), xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[1].set_title(\"CAM\")\nax[2].imshow(grad_cam_superimposed)\nax[2].annotate(\"Max Coherency: \"+tf.strings.as_string(gc[2]).numpy().decode('utf-8')+\"\\nAvg Drop: \" +tf.strings.as_string(gc[0]).numpy().decode('utf-8') + \"\\nAvg Increase: \" + tf.strings.as_string(gc[1]).numpy().decode('utf-8'), xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[2].set_title(\"Grad-CAM\")\nax[3].imshow(smooth_superimposed)\nax[3].annotate(\"Max Coherency: \"+tf.strings.as_string(sgc[2]).numpy().decode('utf-8')+\"\\nAvg Drop: \" +tf.strings.as_string(sgc[0]).numpy().decode('utf-8') + \"\\nAvg Increase: \" + tf.strings.as_string(sgc[1]).numpy().decode('utf-8'), xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[3].set_title(\"Smooth-Grad-CAM\")\nax[4].imshow(score_cam_superimposed)\nax[4].annotate(\"Max Coherency: \"+tf.strings.as_string(sc[2]).numpy().decode('utf-8')+\"\\nAvg Drop: \" +tf.strings.as_string(sc[0]).numpy().decode('utf-8') + \"\\nAvg Increase: \" + tf.strings.as_string(sc[1]).numpy().decode('utf-8'), xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[4].set_title(\"Score-CAM\")\nax[5].imshow(eigen_cam_superimposed)\nax[5].annotate(\"Max Coherency: \"+tf.strings.as_string(ec[2]).numpy().decode('utf-8')+\"\\nAvg Drop: \" +tf.strings.as_string(ec[0]).numpy().decode('utf-8') + \"\\nAvg Increase: \" + tf.strings.as_string(ec[1]).numpy().decode('utf-8'), xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[5].set_title(\"Eigen-CAM\")\nax[6].imshow(svd_grad_cam_superimposed)\nax[6].annotate(\"Max Coherency: \"+tf.strings.as_string(svgc[2]).numpy().decode('utf-8')+\"\\nAvg Drop: \" +tf.strings.as_string(svgc[0]).numpy().decode('utf-8') + \"\\nAvg Increase: \" + tf.strings.as_string(svgc[1]).numpy().decode('utf-8'), xy=(0.5, -0.2), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=12)\nax[6].set_title(\"SVD-Grad-CAM\")\nfor j in range(7):\n    ax[j].axis('off')\nplt.show()\nfig.savefig('Images For Analysis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}